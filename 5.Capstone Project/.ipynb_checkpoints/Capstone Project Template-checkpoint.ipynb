{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Building an ETL Pipeline for Immigration and Temperature Data using Spark\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of the project is to build an ETL pipeline that will be utilised to integrate immigration and temperature data. The developed schema will serve users who seek information on weather-influenced immigration decisions. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Project scope and project data\n",
    "\n",
    "The aim of this project is to use Spark to create an ETL pipeline that will serve users seeking information on weather-influenced immigration statistics.\n",
    "In this project, the I94 immigration data provided and the temperature data from Kaggle will be transformed into two dimension tables respectively and aggregated by destination cities to form the fact table for the schema. With the schema, users can query the database to see if there is a relationshp between cities being migrated to and the the temperature of the cities.\n",
    "\n",
    "The I94 immigration data is souced from the US National Tourism and Trade Office. It is provided in a binary database storage format. Some attributes of the data include: \n",
    "AverageTemperature \n",
    "City \n",
    "Country \n",
    "Latitude \n",
    "Longitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read April 2016 I94 immigration data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immidata = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_label = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "with open(i94_label) as f:\n",
    "    lines = f.readlines()  \n",
    "    \n",
    "#remove this\n",
    "#comments = [line for line in lines if '/*' in line and '*/\\n' in line]\n",
    "#regexp = re.compile(r'^/\\*\\s+(?P<code>.+?)\\s+-\\s+(?P<description>.+)\\s+\\*/$')\n",
    "#matches = [regexp.match(c) for c in comments]\n",
    "\n",
    "#for m in matches:\n",
    "#    print(m.group(\"code\"), \":\", m.group('description'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read temperature data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_tempdata = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tempdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Beginning Spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(df_tempdata)\n",
    "#df_spark=spark.read.parquet(df_tempdata)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploring data and and removing invalid data points\n",
    "\n",
    "Spark is used for processing the data. Data is explored to find missing and invalid data. For I94 immigration data, all data points where i94port is missing or invalid is dropped. For Temperature Data, all data points where AverageTemperature is missing is also dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating valid i94 data\n",
    "re_comp = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "vports = {}\n",
    "for line in lines[302:962]:\n",
    "    match = re_comp.search(line)\n",
    "    vports[match.group(1)]=[match.group(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load I94 data into Spark and clean data\n",
    "def file_to_spark_df(file):\n",
    "    return spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "def prepared_i94(df):\n",
    "    filteredColumns = {'i94yr', 'i94mon', 'i94cit', 'i94port', 'i94mode', 'i94bir', 'arrdate', 'depdate', 'i94visa'}\n",
    "    tempdata = df.filter(df.i94port.isin(list(vports.keys())))\n",
    "    return tempdata.select([c for c in temp.columns if c in filteredColumns])\n",
    "\n",
    "def prepared_i94(file):\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(vports.keys())))\n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "immigration_test_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "df_immigration_test = prepared_i94(immigration_test_file)\n",
    "df_immigration_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cleaning temperature data\n",
    "df_cleantemp = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "df_cleantemp = df_cleantemp.filter(df_cleantemp.AverageTemperature != 'NaN')\n",
    "df_cleantemp = df_cleantemp.dropDuplicates(['City', 'Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Including iport94 based on city name and removing data with no iport94 field\n",
    "@udf()\n",
    "def get_i94port(city):\n",
    "    '''\n",
    "    Input: City name    \n",
    "    Output: Corresponding i94port\n",
    "    \n",
    "    '''\n",
    "    for key in vports:\n",
    "        if city.lower() in vports[key][0].lower():\n",
    "            return key\n",
    "        \n",
    "df_cleantemp = df_cleantemp.withColumn(\"i94port\", get_i94port(df_cleantemp.City))\n",
    "df_cleantemp = df_cleantemp.filter(df_cleantemp.i94port != 'null')\n",
    "df_cleantemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Fact Table: This will have attributes of both the I94 immigration data and the temperature data joined with the city temperature data on i94port.\n",
    "Attributes:\n",
    "i94yr = 4 digit year\n",
    "i94mon = month\n",
    "i94cit = 3 digit code of origin city\n",
    "i94port = 3 character code of destination city\n",
    "i94mode = 1 digit travel code\n",
    "arrdate = arrival date\n",
    "depdate = departure date\n",
    "i94visa = immigration purpose\n",
    "AverageTemperature = average temperature of destination city\n",
    "\n",
    "First dimension table: This contains attributes from the I94 immigration data.\n",
    "Attributes:\n",
    "i94yr = 4 digit year\n",
    "i94mon = month\n",
    "i94cit = 3 digit code of origin city\n",
    "i94port = 3 character code of destination city\n",
    "i94mode = 1 digit travel code\n",
    "arrdate = arrival date\n",
    "depdate = departure date\n",
    "i94visa = immigration purpose\n",
    "\n",
    "Second dimension table: The temperature data from Kaggle which includes.\n",
    "Attributes:\n",
    "AverageTemperature \n",
    "City \n",
    "Country \n",
    "Latitude \n",
    "Longitude\n",
    "i94port = 3 character code of destination city \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps to creating data pipline:\n",
    "\n",
    "I 94 data is cleaned to create Spark dataframe df_immigration for each month.     \n",
    "Temperature data is cleaned to create Spark df_cleanTemp.\n",
    "Immigration dimension table is created by selecting attributes from df_immigration and writing to parquet file partitioned by i94port.\n",
    "Temperature dimension table is created by selecting attributes from df_cleantemp and writing to parquet file partitioned by i94port.\n",
    "Fact table is created by joining immigration and temperature dimension tables on i94port and then writing to parquet file partitioned by i94port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#step1\n",
    "immigration_data = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# Clean I94 immigration data and store as Spark dataframe\n",
    "df_immigration = prepared_i94(immigration_data)\n",
    "# Extract columns for immigration dimension table\n",
    "immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "# Write immigration dimension table to parquet files partitioned by i94port\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#step2\n",
    "# Extract columns for temperature dimension table\n",
    "temp_table = df_cleantemp.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])\n",
    "# Write temperature dimension table to parquet files partitioned by i94port\n",
    "temp_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step 3:\n",
    "# Create temporary views of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "df_cleantemp.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create and write the fact table to i94port parquet files based on i94port\n",
    "fact_table = spark.sql('''\n",
    "SELECT immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94yr as arrival_year,\n",
    "       immigration_view.i94mon as arrival_month,\n",
    "       immigration_view.i94cit as origin_city,\n",
    "       immigration_view.i94port as destin_city,\n",
    "       immigration_view.i94visa as visa,\n",
    "       temp_view.AverageTemperature,\n",
    "       temp_view.Latitude,\n",
    "       temp_view.Longitude \n",
    "FROM immigration_view\n",
    "JOIN temp_view ON (immigration_view.i94port = temp_view.i94port)\n",
    "''')\n",
    "\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing quality checks \n",
    "def data_quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark dataframe\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0\n",
    "\n",
    "data_quality_check(df_immigration, \"immigration table\")\n",
    "data_quality_check(df_cleantemp, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fact Table is a table created by joining I94 immigration data to city temperature data on i94port.\n",
    "immigration_view.arrdate = arrival date\n",
    "immigration_view.depdate = departure date     \n",
    "immigration_view.i94yr = arrival year\n",
    "immigration_view.i94mon = arrival month\n",
    "immigration_view.i94cit = origin city\n",
    "immigration_view.i94port = destinaton city\n",
    "immigration_view.i94visa = visa\n",
    "temp_view.AverageTemperature = average temperature \n",
    "temp_view.Latitude = latitude \n",
    "temp_view.Longitude = longitude \n",
    "\n",
    "Dimension Table 1 contains I94 immigration data.\n",
    "i94yr = arrival year\n",
    "i94mon = arrival month\n",
    "i94cit = origin city\n",
    "i94port = destinaton city\n",
    "arrdate = arrival date\n",
    "i94mode = 1 digit travel code\n",
    "depdate = departure date\n",
    "i94visa = visa\n",
    "\n",
    "Dimension Table 2 contains city temperature data.\n",
    "i94port = destination city from immigration data\n",
    "AverageTemperature = average temperature\n",
    "City = city name\n",
    "Country = country name\n",
    "Latitude= latitude\n",
    "Longitude = longitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Rationale for the choice of tools and technologies.\n",
    "For this project, Panda was used for data exploration because it is customarily the preferred choice due to its efficiency. Then Spark was used in building the data pipeline because it performs well on file formats such as SAS with big data.  \n",
    "\n",
    "\n",
    "Propose how often the data should be updated and why.\n",
    "The data used in creating the schema is captured in montly format so I propose it be updated monthly too. \n",
    "\n",
    "\n",
    "Scenario 1\n",
    "If data is increased by 100x, I would used Redshift to increase efficiency processing efficiency since Redshift is built for handling such big data prcesses efficiently.\n",
    "\n",
    "Scenario 2\n",
    "If the data populates a dashboard that must be updated daily by 7am, I would use Airflow since it enhances management of data pipelines.\n",
    "\n",
    "Scenario 3\n",
    "If the database is to be accessed by 100+ people, I would still use Spark but ensure settings are optimized for large data reads and writes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
